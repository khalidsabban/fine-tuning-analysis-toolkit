# config/config.yaml - QLoRA Configuration for Llama-2
defaults:
  - _self_

model:
  name: NousResearch/Llama-2-7b-chat-hf
  num_labels: 2
  lora_rank: 8  # Can use higher rank with QLoRA & Smaller rank = faster
  use_qlora: true
  quantization_config: "nf4"  # Options: "nf4" (recommended) or "fp4"

training:
  max_steps: -1 # Use epochs instead
  max_epochs: 2   
  batch_size: 4   # Reduced batch size for larger model
  learning_rate: 1e-5  # Slightly higher LR often works better with QLoRA
  gradient_accumulation_steps: 4  # Increased to compensate for smaller batch
  gradient_checkpointing: true
  use_mixed_precision: false

data:
  dataset_name: sst2
  split: "train[:2%]"  # ðŸ”§ FIXED: Reduced to 5% to ensure we have enough steps for debugging
  text_field: "sentence"
  label_field: "label"
  num_workers: 2
  max_length: 512  # Llama can handle longer sequences
  val_split_ratio: 0.1  # ðŸ”§ ADDED: 10% for validation

carbon:
  tracker:
    project_name: llama2_qlora_test
    output_dir: carbon_logs

eval:
  samples:
    - An absolute triumph of storytelling that kept me hooked from start to finish.
    - Brilliant performances and a heartwarming finale make this a mustâ€‘see.
    - A visually stunning masterpiece with an unforgettable score.
    - Heavy on clichÃ©s and light on originality, it feels like a tired retread.
    - Sluggish pacing and wooden acting make this an endurance test.
    - Dialogue so forced it took me right out of the story.

# QLoRA-specific settings
qlora:
  nested_quantization: true  # Enable double quantization for extra memory savings
  compute_dtype: "float16"   # Compute in fp16 for speed
  
# To Fix The Error on Google Colab
hydra:
  run:
    dir: .
