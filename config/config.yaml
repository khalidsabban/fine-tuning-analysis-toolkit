# config/config.yaml - QLoRA Configuration for Llama-2
defaults:
  - _self_

model:
  name: NousResearch/Llama-2-7b-chat-hf
  num_labels: 2
  lora_rank: 8  # Can use higher rank with QLoRA & Smaller rank = faster
  use_qlora: true
  quantization_config: "nf4"  # Options: "nf4" (recommended) or "fp4"

training:
  max_steps: 100 # 1000
  max_epochs: 5
  batch_size: 4  # Reduced batch size for larger model
  learning_rate: 2e-4  # Slightly higher LR often works better with QLoRA
  gradient_accumulation_steps: 4  # Increased to compensate for smaller batch
  gradient_checkpointing: true
  use_mixed_precision: true

data:
  dataset_name: imdb
  split: "train[:25%]"  # Can use more data with QLoRA
  text_field: "text"
  label_field: "label"
  num_workers: 2
  max_length: 512  # Llama can handle longer sequences

carbon:
  tracker:
    project_name: llama2_qlora_test
    output_dir: carbon_logs

eval:
  samples:
    - "My name is khalid."
    - "I did not enjoy the film."
    - "This movie was fantastic!"
    - "I think I will not watch it again."

# QLoRA-specific settings
qlora:
  nested_quantization: true  # Enable double quantization for extra memory savings
  compute_dtype: "float16"   # Compute in fp16 for speed
  
# To Fix The Error on Google Colab
hydra:
  run:
    dir: .
    