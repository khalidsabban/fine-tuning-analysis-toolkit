# config/config.yaml - QLoRA Configuration for Llama-2
defaults:
  - _self_

model:
  name: NousResearch/Llama-2-7b-chat-hf
  num_labels: 2
  lora_rank: 8  # Can use higher rank with QLoRA & Smaller rank = faster
  use_qlora: true
  quantization_config: "nf4"  # Options: "nf4" (recommended) or "fp4"

training:
  max_steps: -1 # Use epochs instead
  max_epochs: 2   
  batch_size: 4   # Reduced batch size for larger model
  learning_rate: 1e-5  # Slightly higher LR often works better with QLoRA
  gradient_accumulation_steps: 4  # Increased to compensate for smaller batch
  gradient_checkpointing: true
  use_mixed_precision: false

data:
  dataset_name: sst2
  split: "train[:5%]"  # ðŸ”§ FIXED: Reduced to 5% to ensure we have enough steps for debugging
  text_field: "sentence"
  label_field: "label"
  num_workers: 2
  max_length: 512  # Llama can handle longer sequences
  val_split_ratio: 0.1  # ðŸ”§ ADDED: 10% for validation

carbon:
  tracker:
    project_name: llama2_qlora_test
    output_dir: carbon_logs

eval:
  samples:
    - "My name is khalid."
    - "I did not enjoy the film."
    - "This movie was fantastic!"
    - "I think I will not watch it again."

# QLoRA-specific settings
qlora:
  nested_quantization: true  # Enable double quantization for extra memory savings
  compute_dtype: "float16"   # Compute in fp16 for speed
  
# To Fix The Error on Google Colab
hydra:
  run:
    dir: .
