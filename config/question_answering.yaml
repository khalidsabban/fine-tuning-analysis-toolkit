# config/question_answering.yaml - Memory-Optimized QA Configuration for Colab
defaults:
  - config

# Override task type
task:
  type: "question_answering"

# Memory-optimized model settings
model:
  name: NousResearch/Llama-2-7b-hf #skeskinen/llama-lite-134m
  lora_rank: 8  # Reduced from 8 to save memory
  lora_alpha: 16 # For better stability
  lora_dropout: 0.1 # Prevent overfitting
  quantization_config: "fp4"  # More aggressive quantization

# QA dataset with very small size for Colab
data:
  dataset_name: "squad"
  split: "train[:2%]"  # Much smaller dataset
  question_field: "question"
  context_field: "context"
  answers_field: "answers"
  max_length: 256
  val_split_ratio: 0.1

# Memory-optimized training settings
training:
  max_steps: 500
  max_epochs: -1
  batch_size: 1  # Perfect
  gradient_accumulation_steps: 4  # Reduced from 16 for faster iteration
  learning_rate: 2e-4  # Slightly higher for LoRA
  gradient_checkpointing: true
  use_mixed_precision: true
  # Add this for memory optimization
  optim: "paged_adamw_8bit"  # 8-bit optimizer

# QA-specific evaluation samples
eval:
  qa_samples:
    - question: "What is the capital of France?"
      context: "France is a country in Western Europe. Paris is the capital and largest city of France. The city has a population of over 2 million people."
    - question: "Who wrote Romeo and Juliet?"
      context: "William Shakespeare was an English playwright and poet. He wrote many famous plays including Romeo and Juliet, Hamlet, and Macbeth during the late 16th and early 17th centuries."
    - question: "When was the Declaration of Independence signed?"
      context: "The Declaration of Independence was signed on July 4, 1776, in Philadelphia. It declared the thirteen American colonies independent from British rule."

carbon:
  tracker:
    project_name: llama2_qlora_qa_test_colab
