# config/question_answering.yaml - Memory-Optimized QA Configuration for Colab
defaults:
  - config

# Override task type
task:
  type: "question_answering"

# Memory-optimized model settings
model:
  lora_rank: 4  # Reduced from 8 to save memory
  quantization_config: "fp4"  # More aggressive quantization

# QA dataset with very small size for Colab
data:
  dataset_name: "squad"
  split: "train[:1%]"  # Much smaller dataset
  question_field: "question"
  context_field: "context"
  answers_field: "answers"
  max_length: 256  # Shorter sequences to save memory
  val_split_ratio: 0.1

# Memory-optimized training settings
training:
  batch_size: 1  # Minimum batch size
  gradient_accumulation_steps: 16  # Compensate with more accumulation
  learning_rate: 3e-5
  max_epochs: 1  # Just one epoch for testing
  gradient_checkpointing: true
  use_mixed_precision: true

# QA-specific evaluation samples
eval:
  qa_samples:
    - question: "What is the capital of France?"
      context: "France is a country in Western Europe. Paris is the capital and largest city of France. The city has a population of over 2 million people."
    - question: "Who wrote Romeo and Juliet?"
      context: "William Shakespeare was an English playwright and poet. He wrote many famous plays including Romeo and Juliet, Hamlet, and Macbeth during the late 16th and early 17th centuries."
    - question: "When was the Declaration of Independence signed?"
      context: "The Declaration of Independence was signed on July 4, 1776, in Philadelphia. It declared the thirteen American colonies independent from British rule."

carbon:
  tracker:
    project_name: llama2_qlora_qa_test_colab
    